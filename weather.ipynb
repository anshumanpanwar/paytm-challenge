{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+\n",
      "|STN---| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+\n",
      "| 10260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7| 29.8|21.7*|0.02G| 18.5|  1000|\n",
      "| 10260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|27.1*| 20.7|0.48G| 22.8|  1000|\n",
      "| 10260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|37.4*|26.8*|0.25G|999.9| 11000|\n",
      "| 10260|99999|20190104|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8|10.9| 36.1| 31.8|0.52G|999.9|  1000|\n",
      "| 10260|99999|20190105|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6|21.0|38.5*| 32.7|0.02G| 23.6| 10000|\n",
      "| 10260|99999|20190106|38.5|34.1|1008.2| 994.2| 12.8|10.0| 17.5|28.9| 41.4|33.8*|0.12G| 23.2| 10000|\n",
      "| 10260|99999|20190107|32.1|29.8| 996.8| 982.7|  6.9|11.3| 15.5|28.6|35.1*| 30.4|0.00G|999.9|  1000|\n",
      "| 10260|99999|20190108|31.6|28.0| 997.4| 983.3| 22.9| 5.9| 11.7|19.0| 34.3|28.0*|0.53G|  0.4| 11000|\n",
      "| 10260|99999|20190109|29.9|27.7|1011.6| 997.3| 29.8| 7.6| 15.2|26.6| 32.4| 26.1|0.20G| 23.6|  1000|\n",
      "| 10260|99999|20190110|33.1|30.6| 979.1| 965.3|  5.3|17.8| 24.9|41.8| 41.4|28.8*|0.00G|999.9| 11000|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv([\\\n",
    "'data/2019/part-00000-890686c0-c142-4c69-a744-dfdc9eca7df4-c000.csv.gz',\\\n",
    "'data/2019/part-00001-890686c0-c142-4c69-a744-dfdc9eca7df4-c000.csv.gz',\\\n",
    "'data/2019/part-00002-890686c0-c142-4c69-a744-dfdc9eca7df4-c000.csv.gz',\\\n",
    "'data/2019/part-00003-890686c0-c142-4c69-a744-dfdc9eca7df4-c000.csv.gz',\\\n",
    "'data/2019/part-00004-890686c0-c142-4c69-a744-dfdc9eca7df4-c000.csv.gz']\\\n",
    "                ,header=True, inferSchema=True) \n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COUNTRY_ABBR: string (nullable = true)\n",
      " |-- STN_NO: integer (nullable = true)\n",
      " |-- WBAN: integer (nullable = true)\n",
      " |-- YEARMODA: integer (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: string (nullable = true)\n",
      " |-- MIN: string (nullable = true)\n",
      " |-- PRCP: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHTT: integer (nullable = true)\n",
      " |-- COUNTRY_FULL: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change column name to make it easier to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+\n",
      "|STN_NO| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+\n",
      "| 10260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7| 29.8|21.7*|0.02G| 18.5|  1000|\n",
      "| 10260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|27.1*| 20.7|0.48G| 22.8|  1000|\n",
      "| 10260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|37.4*|26.8*|0.25G|999.9| 11000|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumnRenamed('STN---', 'STN_NO')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|STN_NO|COUNTRY_ABBR|\n",
      "+------+------------+\n",
      "|012240|          NO|\n",
      "|020690|          SW|\n",
      "|020870|          SW|\n",
      "|021190|          SW|\n",
      "|032690|          UK|\n",
      "+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations=spark.read.csv(\"stationlist.csv\", header=True, inferSchema=True)\n",
    "stations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4158416\n"
     ]
    }
   ],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+------------+\n",
      "|STN_NO| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|COUNTRY_ABBR|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+------------+\n",
      "| 10260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7| 29.8|21.7*|0.02G| 18.5|  1000|          NO|\n",
      "| 10260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|27.1*| 20.7|0.48G| 22.8|  1000|          NO|\n",
      "| 10260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|37.4*|26.8*|0.25G|999.9| 11000|          NO|\n",
      "| 10260|99999|20190104|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8|10.9| 36.1| 31.8|0.52G|999.9|  1000|          NO|\n",
      "| 10260|99999|20190105|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6|21.0|38.5*| 32.7|0.02G| 23.6| 10000|          NO|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.join(stations, [\"STN_NO\"], \"left\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|COUNTRY_ABBR|       COUNTRY_FULL|\n",
      "+------------+-------------------+\n",
      "|          AA|              ARUBA|\n",
      "|          AC|ANTIGUA AND BARBUDA|\n",
      "|          AF|        AFGHANISTAN|\n",
      "|          AG|            ALGERIA|\n",
      "|          AI|   ASCENSION ISLAND|\n",
      "+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries=spark.read.csv(\"countrylist.csv\", header=True, inferSchema=True)\n",
    "countries.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+------------+\n",
      "|COUNTRY_ABBR|STN_NO| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|COUNTRY_FULL|\n",
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+------------+\n",
      "|          NO| 10260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7| 29.8|21.7*|0.02G| 18.5|  1000|      NORWAY|\n",
      "|          NO| 10260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|27.1*| 20.7|0.48G| 22.8|  1000|      NORWAY|\n",
      "|          NO| 10260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|37.4*|26.8*|0.25G|999.9| 11000|      NORWAY|\n",
      "|          NO| 10260|99999|20190104|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8|10.9| 36.1| 31.8|0.52G|999.9|  1000|      NORWAY|\n",
      "|          NO| 10260|99999|20190105|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6|21.0|38.5*| 32.7|0.02G| 23.6| 10000|      NORWAY|\n",
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.join(countries, [\"COUNTRY_ABBR\"], \"left\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "Query some random columns to get a sense of which all data is missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----+--------+----+----+---+---+-----+----+-----+----+---+---+----+----+------+------------+\n",
      "|COUNTRY_ABBR|STN_NO|WBAN|YEARMODA|TEMP|DEWP|SLP|STP|VISIB|WDSP|MXSPD|GUST|MAX|MIN|PRCP|SNDP|FRSHTT|COUNTRY_FULL|\n",
      "+------------+------+----+--------+----+----+---+---+-----+----+-----+----+---+---+----+----+------+------------+\n",
      "+------------+------+----+--------+----+----+---+---+-----+----+-----+----+---+---+----+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.TEMP ==9999.9) ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is between -114.7 and 110.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean as _mean, min as _min, max as _max,col\n",
    "\n",
    "df_stats = df.select(\n",
    "    _min(col('TEMP')).alias('min'),\n",
    "    _max(col('TEMP')).alias('max'),\n",
    ").collect()\n",
    "\n",
    "print('Data is between' ,df_stats[0]['min'], 'and', df_stats[0]['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----+--------+----+------+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+------------+\n",
      "|COUNTRY_ABBR|STN_NO| WBAN|YEARMODA|TEMP|  DEWP|   SLP|   STP|VISIB|WDSP|MXSPD| GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|COUNTRY_FULL|\n",
      "+------------+------+-----+--------+----+------+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+------------+\n",
      "|          SW| 22190|99999|20190105|33.8|9999.9|1021.1| 953.2|  5.6| 5.2|  7.8| 15.3| 35.4| 33.1|0.13G|999.9|110000|      SWEDEN|\n",
      "|          NO| 13730|99999|20191124|32.7|9999.9|1021.4|1000.4|999.9| 0.8|  1.9|999.9|33.3*|32.4*|0.00G|999.9|100000|      NORWAY|\n",
      "|          SW| 24310|99999|20191128|30.4|9999.9| 988.6| 954.2|  2.2| 2.8|  5.8| 11.3| 32.0|25.7*|0.19G|999.9|  1000|      SWEDEN|\n",
      "|          SW| 24310|99999|20191129|24.9|9999.9| 995.6| 960.8| 24.3| 4.2|  7.8| 16.9| 28.8| 23.4|0.29G|999.9|  1000|      SWEDEN|\n",
      "|          SW| 24310|99999|20191130|23.0|9999.9|1012.1| 976.6| 29.9| 3.5|  6.4| 17.3| 28.2|18.3*|0.00G|999.9|     0|      SWEDEN|\n",
      "+------------+------+-----+--------+----+------+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.DEWP ==9999.9) ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----+--------+----+----+------+-----+-----+----+-----+-----+-----+-----+-----+-----+------+------------+\n",
      "|COUNTRY_ABBR|STN_NO| WBAN|YEARMODA|TEMP|DEWP|   SLP|  STP|VISIB|WDSP|MXSPD| GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|COUNTRY_FULL|\n",
      "+------------+------+-----+--------+----+----+------+-----+-----+----+-----+-----+-----+-----+-----+-----+------+------------+\n",
      "|          SI|140080|99999|20190101|25.3|12.3|9999.9|750.0| 49.1|16.8| 22.5| 38.7| 31.8| 18.7|0.00A|999.9|     0|    SLOVENIA|\n",
      "|          SI|140080|99999|20190102| 7.8| 1.4|9999.9|741.5| 28.5|21.8| 35.0| 64.3|18.7*|-0.2*|0.02G|  9.8|101000|    SLOVENIA|\n",
      "|          SI|140080|99999|20190103| 0.3|-7.0|9999.9|742.5| 41.0|25.2| 40.2| 70.5|  2.1| -0.8|0.00G|  9.8|101000|    SLOVENIA|\n",
      "|          SI|140080|99999|20190104| 3.3|-1.0|9999.9|745.3| 35.4|16.8| 26.4| 52.1|  9.1| -0.9|0.00G|  9.8|101000|    SLOVENIA|\n",
      "|          SI|140080|99999|20190105|15.1| 9.1|9999.9|743.0| 46.9|20.5| 31.1|999.9| 19.8|  6.4|0.03G|  9.8|  1000|    SLOVENIA|\n",
      "|          SI|140080|99999|20190106|15.3| 8.8|9999.9|744.4| 41.2|22.8| 33.0|999.9|17.4*|13.6*|0.00G|  9.8|  1000|    SLOVENIA|\n",
      "|          SI|140080|99999|20190107|14.8| 9.7|9999.9|747.6| 43.2|19.1| 27.2| 55.2|17.2*| 13.3|0.00G|  9.8|  1000|    SLOVENIA|\n",
      "|          SI|140080|99999|20190108|16.8| 8.4|9999.9|740.4| 33.2|18.6| 31.1| 47.4| 22.6|12.2*|0.00G| 11.8|  1000|    SLOVENIA|\n",
      "|          SI|140080|99999|20190109| 8.3| 2.7|9999.9|732.2| 35.1|17.1| 32.4| 54.6|11.8*| 5.2*|0.13G| 12.2|  1000|    SLOVENIA|\n",
      "|          SI|140080|99999|20190110| 5.7| 1.8|9999.9|735.5| 17.0|17.1| 26.2| 47.8|  7.0|  4.5|0.01G|  9.8|101000|    SLOVENIA|\n",
      "+------------+------+-----+--------+----+----+------+-----+-----+----+-----+-----+-----+-----+-----+-----+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.SLP ==9999.9) ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+------------+\n",
      "|COUNTRY_ABBR|STN_NO| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD| GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|COUNTRY_FULL|\n",
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+------------+\n",
      "|          NO| 10260|99999|20190116|24.8|21.8| 992.5| 978.2| 33.4| 2.7|  5.8|999.9| 26.1| 23.5|0.06G| 35.4|  1000|      NORWAY|\n",
      "|          NO| 10260|99999|20190124|14.4|11.5|1019.1|1004.2| 44.7| 2.6|  3.9|999.9|17.2*| 12.2|0.00G| 40.9|     0|      NORWAY|\n",
      "|          NO| 10260|99999|20190208|18.3|16.5| 997.8| 983.3| 45.4| 2.4|  5.8|999.9|24.8*| 14.4|0.00G| 37.8|     0|      NORWAY|\n",
      "|          NO| 10260|99999|20190209|15.5|13.8| 993.4| 978.9| 41.6| 1.8|  3.7|999.9| 21.6|13.1*|0.00G| 37.4|     0|      NORWAY|\n",
      "|          NO| 10260|99999|20190314|27.2|20.6| 990.4| 976.2| 22.1| 3.3|  5.8|999.9| 34.0|23.7*|0.07G|999.9|  1000|      NORWAY|\n",
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.GUST ==999.9) ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201147\n"
     ]
    }
   ],
   "source": [
    "print(df.filter((df.DEWP ==9999.9) ).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a replace function and replace all Missing data with Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------------+\n",
      "|COUNTRY_ABBR|STN_NO| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST| MAX| MIN|PRCP|SNDP|FRSHTT|COUNTRY_FULL|\n",
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------------+\n",
      "|          NO| 10260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7|29.8|null|null|18.5|  1000|      NORWAY|\n",
      "|          NO| 10260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|null|20.7|null|22.8|  1000|      NORWAY|\n",
      "|          NO| 10260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|null|null|null|null| 11000|      NORWAY|\n",
      "|          NO| 10260|99999|20190104|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8|10.9|36.1|31.8|null|null|  1000|      NORWAY|\n",
      "|          NO| 10260|99999|20190105|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6|21.0|null|32.7|null|23.6| 10000|      NORWAY|\n",
      "|          NO| 10260|99999|20190106|38.5|34.1|1008.2| 994.2| 12.8|10.0| 17.5|28.9|41.4|null|null|23.2| 10000|      NORWAY|\n",
      "|          NO| 10260|99999|20190107|32.1|29.8| 996.8| 982.7|  6.9|11.3| 15.5|28.6|null|30.4|null|null|  1000|      NORWAY|\n",
      "|          NO| 10260|99999|20190108|31.6|28.0| 997.4| 983.3| 22.9| 5.9| 11.7|19.0|34.3|null|null| 0.4| 11000|      NORWAY|\n",
      "|          NO| 10260|99999|20190109|29.9|27.7|1011.6| 997.3| 29.8| 7.6| 15.2|26.6|32.4|26.1|null|23.6|  1000|      NORWAY|\n",
      "|          NO| 10260|99999|20190110|33.1|30.6| 979.1| 965.3|  5.3|17.8| 24.9|41.8|41.4|null|null|null| 11000|      NORWAY|\n",
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "def replace1(column, value):\n",
    "    return when(column != value, column).otherwise(lit(None))\n",
    "\n",
    "df=df.withColumn(\"TEMP\", replace1(col(\"TEMP\"), 9999.9)).\\\n",
    "withColumn(\"DEWP\", replace1(col(\"DEWP\"), 9999.9)).\\\n",
    "withColumn(\"SLP\", replace1(col(\"SLP\"), 9999.9)).\\\n",
    "withColumn(\"STP\", replace1(col(\"STP\"), 9999.9)).\\\n",
    "withColumn(\"VISIB\", replace1(col(\"VISIB\"), 999.9)).\\\n",
    "withColumn(\"WDSP\", replace1(col(\"WDSP\"), 999.9)).\\\n",
    "withColumn(\"MXSPD\", replace1(col(\"MXSPD\"), 999.9)).\\\n",
    "withColumn(\"GUST\", replace1(col(\"GUST\"), 999.9)).\\\n",
    "withColumn(\"MAX\", replace1(col(\"MAX\"), 9999.9)).\\\n",
    "withColumn(\"MIN\", replace1(col(\"MIN\"), 9999.9)).\\\n",
    "withColumn(\"PRCP\", replace1(col(\"PRCP\"), 99.9)).\\\n",
    "withColumn(\"SNDP\", replace1(col(\"SNDP\"), 999.9))\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Which country had the hottest average mean temperature over the year?\n",
    "We will first calculate the average mean temp for each station and then aggregate over the country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------------+\n",
      "|STN_NO| COUNTRY_FULL| mean_temp_station|\n",
      "+------+-------------+------------------+\n",
      "|727755|UNITED STATES| 41.55601092896176|\n",
      "|876450|    ARGENTINA| 56.35643835616438|\n",
      "|110220|      AUSTRIA|52.682142857142814|\n",
      "|726435|UNITED STATES|43.326502732240414|\n",
      "|270370|       RUSSIA| 40.50821917808217|\n",
      "|889630|   ANTARCTICA| 23.83972602739725|\n",
      "|713070|       CANADA|47.952876712328774|\n",
      "|122800|       POLAND|49.255616438356135|\n",
      "| 42340|    GREENLAND|29.745604395604396|\n",
      "| 13850|       NORWAY|41.818356164383566|\n",
      "+------+-------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count, first, last, countDistinct\n",
    "grpdf1=df.groupby('STN_NO', 'COUNTRY_FULL')\\\n",
    "    .agg(avg(\"TEMP\").alias(\"mean_temp_station\"))\n",
    "grpdf1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets aggregate over country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|        COUNTRY_FULL|mean_temp_country|\n",
      "+--------------------+-----------------+\n",
      "|            DJIBOUTI|90.06114457831325|\n",
      "|               SUDAN|88.88602786294095|\n",
      "|                CHAD| 88.1159782741869|\n",
      "|               NIGER|85.06016337723437|\n",
      "|         EL SALVADOR|  84.539789278576|\n",
      "| JUAN DE NOVA ISLAND|84.15945945945947|\n",
      "|             TOKELAU|83.96142857142854|\n",
      "|BRITISH INDIAN OC...|83.91150684931505|\n",
      "|       GUINEA-BISSAU|83.79837994935542|\n",
      "|      CAYMAN ISLANDS|83.77186578336705|\n",
      "|        BURKINA FASO|83.76038111925736|\n",
      "|            MALDIVES|83.67879452054797|\n",
      "|         GAMBIA  THE|83.64384615384616|\n",
      "|           SINGAPORE|83.63278538812786|\n",
      "|              TUVALU|83.51711850051552|\n",
      "|            CAMBODIA|83.49888303477346|\n",
      "|          MICRONESIA|83.43944358080198|\n",
      "|            KIRIBATI| 83.3898426671738|\n",
      "|ST. VINCENT AND T...|83.28311111111111|\n",
      "|             COMOROS|83.27314788929272|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grpdf2=grpdf1.groupby('COUNTRY_FULL')\\\n",
    "    .agg(avg(\"mean_temp_station\").alias(\"mean_temp_country\"))\n",
    "grpdf2.sort(grpdf2.mean_temp_country.desc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer DJIBOUTI has the hottest average mean temperature over the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Which country had the second highest average mean wind speed over the year?\n",
    "Similar to above we will first calculate the average mean temp for each station and then aggregate over the country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------------------+\n",
      "|STN_NO| COUNTRY_FULL|mean_windspeed_station|\n",
      "+------+-------------+----------------------+\n",
      "|727755|UNITED STATES|     8.314207650273223|\n",
      "|876450|    ARGENTINA|      7.92438356164384|\n",
      "|110220|      AUSTRIA|     7.207417582417587|\n",
      "|726435|UNITED STATES|     6.211202185792346|\n",
      "|270370|       RUSSIA|     5.137087912087906|\n",
      "|889630|   ANTARCTICA|    13.710684931506844|\n",
      "|713070|       CANADA|      6.57616438356164|\n",
      "|122800|       POLAND|     7.201643835616438|\n",
      "| 42340|    GREENLAND|     6.426923076923077|\n",
      "| 13850|       NORWAY|     4.076438356164382|\n",
      "+------+-------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count, first, last, countDistinct\n",
    "grpdf3=df.groupby('STN_NO', 'COUNTRY_FULL')\\\n",
    "    .agg(avg(\"WDSP\").alias(\"mean_windspeed_station\"))\n",
    "grpdf3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets aggregate over country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "|        COUNTRY_FULL|mean_windspeed_country|\n",
      "+--------------------+----------------------+\n",
      "|FALKLAND ISLANDS ...|    17.757090064203215|\n",
      "|               ARUBA|    15.975683060109283|\n",
      "|       FAROE ISLANDS|    15.975523290644825|\n",
      "|FRENCH SOUTHERN A...|    15.764206899383856|\n",
      "|            BARBADOS|    14.097540983606562|\n",
      "|ST. PIERRE AND MI...|     13.90767123287671|\n",
      "|          CAPE VERDE|    13.723272057202525|\n",
      "|     TROMELIN ISLAND|    13.005277777777751|\n",
      "|          ST. HELENA|    12.727092749187655|\n",
      "|             SOMALIA|    12.244368239674623|\n",
      "|          ANTARCTICA|    12.083258399284333|\n",
      "|COCOS (KEELING) I...|    12.054545454545453|\n",
      "|            GUERNSEY|    12.021826610156673|\n",
      "|        MAN  ISLE OF|    11.893424657534247|\n",
      "|ST. VINCENT AND T...|    11.862222222222222|\n",
      "|          MONTSERRAT|    11.808813559322036|\n",
      "|             ICELAND|     11.69694194369551|\n",
      "|      WESTERN SAHARA|    11.629203249998602|\n",
      "|           ST. LUCIA|    11.569904558724453|\n",
      "|            SVALBARD|    11.536456790720214|\n",
      "+--------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grpdf4=grpdf3.groupby('COUNTRY_FULL')\\\n",
    "    .agg(avg(\"mean_windspeed_station\").alias(\"mean_windspeed_country\"))\n",
    "grpdf4.sort(grpdf4.mean_windspeed_country.desc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer ARUBA had the second highest average mean wind speed over the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Which country had the most consecutive days of tornadoes/funnel cloud formations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last digit in FRSHTT sgnifies tornado formation. It can be extracted by taking %10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------------+-------+\n",
      "|COUNTRY_ABBR|STN_NO| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST| MAX| MIN|PRCP|SNDP|FRSHTT|COUNTRY_FULL|tornado|\n",
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------------+-------+\n",
      "|          NO| 10260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7|29.8|null|null|18.5|  1000|      NORWAY|      0|\n",
      "|          NO| 10260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|null|20.7|null|22.8|  1000|      NORWAY|      0|\n",
      "|          NO| 10260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|null|null|null|null| 11000|      NORWAY|      0|\n",
      "|          NO| 10260|99999|20190104|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8|10.9|36.1|31.8|null|null|  1000|      NORWAY|      0|\n",
      "|          NO| 10260|99999|20190105|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6|21.0|null|32.7|null|23.6| 10000|      NORWAY|      0|\n",
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"tornado\", df.FRSHTT % 10)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------------+-------+\n",
      "|COUNTRY_ABBR|STN_NO| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST| MAX| MIN|PRCP|SNDP|FRSHTT|COUNTRY_FULL|tornado|\n",
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------------+-------+\n",
      "|          AU|110300|99999|20190616|71.3|60.4|1015.0| 994.4| 21.6|11.0| 23.3|null|76.3|null|null|null|    11|     AUSTRIA|      1|\n",
      "|          AU|110350|99999|20190619|74.5|62.5|1011.5| 987.8| 15.9| 4.8|  7.8|null|86.9|64.0|null|null|    11|     AUSTRIA|      1|\n",
      "|          NO| 10280|99999|20191030|21.7|15.1|1014.8|1012.7| 22.9| 8.8| 15.5|null|null|17.4|null|null|     1|      NORWAY|      1|\n",
      "|          CA|718270|99999|20190616|38.1|37.2|1006.8| 981.9|  3.2|13.3| 17.1|21.0|null|null|null|null|110001|      CANADA|      1|\n",
      "|          NO| 14920|99999|20191127|35.1|32.5| 999.5| 987.6| 23.8| 8.0|  9.7|null|36.0|34.0|null|null| 11001|      NORWAY|      1|\n",
      "+------------+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.tornado ==1) ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate (sum) over date and country to find if there was tornado on the date in the country at any station\n",
    "Tornado on that day if sum>1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------------------+\n",
      "|YEARMODA|COUNTRY_FULL|number_of_tornado_on_day|\n",
      "+--------+------------+------------------------+\n",
      "|20190422|      NORWAY|                       0|\n",
      "|20191002|      NORWAY|                       0|\n",
      "|20190326|    SLOVENIA|                       0|\n",
      "|20190907|    SLOVENIA|                       0|\n",
      "|20190422|     AUSTRIA|                       0|\n",
      "|20190724|     AUSTRIA|                       0|\n",
      "|20190515|     FINLAND|                       0|\n",
      "|20190715|     FINLAND|                       0|\n",
      "|20190725|     FINLAND|                       0|\n",
      "|20190917|     FINLAND|                       0|\n",
      "+--------+------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count, first, last, countDistinct\n",
    "grpdf5=df.groupby('YEARMODA', 'COUNTRY_FULL')\\\n",
    "    .agg(sum(\"tornado\").alias(\"number_of_tornado_on_day\"))\n",
    "grpdf5.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------------------+\n",
      "|YEARMODA|  COUNTRY_FULL|number_of_tornado_on_day|\n",
      "+--------+--------------+------------------------+\n",
      "|20191209| UNITED STATES|                       2|\n",
      "|20190811|        CANADA|                       2|\n",
      "|20191109|         ITALY|                       2|\n",
      "|20190827| UNITED STATES|                       2|\n",
      "|20190421|CAYMAN ISLANDS|                       2|\n",
      "+--------+--------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grpdf5.filter((grpdf5.number_of_tornado_on_day >1) ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace with 1 if number_of_tornado_on_day>0 (to mark there was a tornado in the country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------------------+\n",
      "|YEARMODA|COUNTRY_FULL|number_of_tornado_on_day|\n",
      "+--------+------------+------------------------+\n",
      "|20190422|      NORWAY|                       0|\n",
      "|20191002|      NORWAY|                       0|\n",
      "|20190326|    SLOVENIA|                       0|\n",
      "|20190907|    SLOVENIA|                       0|\n",
      "|20190422|     AUSTRIA|                       0|\n",
      "+--------+------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "def replace2(column):\n",
    "    return when(column > 0, 1).otherwise(0)\n",
    "\n",
    "grpdf5=grpdf5.withColumn(\"number_of_tornado_on_day\", replace2(col(\"number_of_tornado_on_day\")))\n",
    "grpdf5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------------------+\n",
      "|YEARMODA|COUNTRY_FULL|number_of_tornado_on_day|\n",
      "+--------+------------+------------------------+\n",
      "+--------+------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grpdf5.filter((grpdf5.number_of_tornado_on_day >1) ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum the consecutive days for every country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------------------+----------------------------+\n",
      "|YEARMODA|COUNTRY_FULL|number_of_tornado_on_day|sum_number_of_tornado_on_day|\n",
      "+--------+------------+------------------------+----------------------------+\n",
      "|20190101|     ARMENIA|                       0|                           0|\n",
      "|20190102|     ARMENIA|                       0|                           0|\n",
      "|20190103|     ARMENIA|                       0|                           0|\n",
      "|20190104|     ARMENIA|                       0|                           0|\n",
      "|20190105|     ARMENIA|                       0|                           0|\n",
      "|20190106|     ARMENIA|                       0|                           0|\n",
      "|20190107|     ARMENIA|                       0|                           0|\n",
      "|20190108|     ARMENIA|                       0|                           0|\n",
      "|20190109|     ARMENIA|                       0|                           0|\n",
      "|20190110|     ARMENIA|                       0|                           0|\n",
      "+--------+------------+------------------------+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "grpdf5 = grpdf5.withColumn(\"sum_number_of_tornado_on_day\", sum('number_of_tornado_on_day').over(Window.partitionBy('COUNTRY_FULL').orderBy('YEARMODA')))\n",
    "grpdf5.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------------------------+----------------------------+\n",
      "|YEARMODA| COUNTRY_FULL|number_of_tornado_on_day|sum_number_of_tornado_on_day|\n",
      "+--------+-------------+------------------------+----------------------------+\n",
      "|20190101|UNITED STATES|                       0|                           0|\n",
      "|20190102|UNITED STATES|                       0|                           0|\n",
      "|20190103|UNITED STATES|                       0|                           0|\n",
      "|20190104|UNITED STATES|                       0|                           0|\n",
      "|20190105|UNITED STATES|                       0|                           0|\n",
      "|20190106|UNITED STATES|                       0|                           0|\n",
      "|20190107|UNITED STATES|                       0|                           0|\n",
      "|20190108|UNITED STATES|                       0|                           0|\n",
      "|20190109|UNITED STATES|                       0|                           0|\n",
      "|20190110|UNITED STATES|                       0|                           0|\n",
      "|20190111|UNITED STATES|                       0|                           0|\n",
      "|20190112|UNITED STATES|                       1|                           1|\n",
      "|20190113|UNITED STATES|                       1|                           2|\n",
      "|20190114|UNITED STATES|                       0|                           2|\n",
      "|20190115|UNITED STATES|                       0|                           2|\n",
      "|20190116|UNITED STATES|                       0|                           2|\n",
      "|20190117|UNITED STATES|                       0|                           2|\n",
      "|20190118|UNITED STATES|                       0|                           2|\n",
      "|20190119|UNITED STATES|                       0|                           2|\n",
      "|20190120|UNITED STATES|                       0|                           2|\n",
      "+--------+-------------+------------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grpdf5.filter((grpdf5.COUNTRY_FULL =='UNITED STATES') ).sort(grpdf5.YEARMODA).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COunt how many time same value of 'sum_number_of_tornado_on_day' appeared "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------------+------------------+\n",
      "|        COUNTRY_FULL|sum_number_of_tornado_on_day|count_cons_tornado|\n",
      "+--------------------+----------------------------+------------------+\n",
      "|             ARMENIA|                           0|               365|\n",
      "|        SOUTH AFRICA|                           0|               365|\n",
      "|               BURMA|                           0|               365|\n",
      "|            CAMBODIA|                           0|               365|\n",
      "|          BANGLADESH|                           0|               233|\n",
      "|          BANGLADESH|                           1|               132|\n",
      "|               JAPAN|                           0|                16|\n",
      "|               JAPAN|                           1|                 7|\n",
      "|               JAPAN|                           2|                37|\n",
      "|               JAPAN|                           3|                31|\n",
      "|               JAPAN|                           4|                69|\n",
      "|               JAPAN|                           5|                 1|\n",
      "|               JAPAN|                           6|                60|\n",
      "|               JAPAN|                           7|                20|\n",
      "|               JAPAN|                           8|                95|\n",
      "|               JAPAN|                           9|                 1|\n",
      "|               JAPAN|                          10|                10|\n",
      "|               JAPAN|                          11|                18|\n",
      "|              UGANDA|                           0|               357|\n",
      "|SOUTH GEORGIA AND...|                           0|               365|\n",
      "+--------------------+----------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grpdf6=grpdf5.groupby('COUNTRY_FULL', 'sum_number_of_tornado_on_day')\\\n",
    "    .agg(count(\"sum_number_of_tornado_on_day\").alias(\"count_cons_tornado\"))\n",
    "grpdf6.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------+------------------+\n",
      "| COUNTRY_FULL|sum_number_of_tornado_on_day|count_cons_tornado|\n",
      "+-------------+----------------------------+------------------+\n",
      "|UNITED STATES|                           1|                 1|\n",
      "|UNITED STATES|                          16|                 1|\n",
      "|UNITED STATES|                           9|                 2|\n",
      "|UNITED STATES|                          12|                 2|\n",
      "|UNITED STATES|                           8|                 3|\n",
      "|UNITED STATES|                          27|                 3|\n",
      "|UNITED STATES|                          15|                 3|\n",
      "|UNITED STATES|                           7|                 3|\n",
      "|UNITED STATES|                          13|                 4|\n",
      "|UNITED STATES|                           6|                 4|\n",
      "+-------------+----------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grpdf6.filter((grpdf6.COUNTRY_FULL =='UNITED STATES') ).sort(grpdf6.count_cons_tornado).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter values where count_cons_tornado is 1\n",
    "The count keeps changing as consecutive days hae tornado=1. So count==1 will give the max number of consecutive days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------------+------------------+\n",
      "|  COUNTRY_FULL|sum_number_of_tornado_on_day|count_cons_tornado|\n",
      "+--------------+----------------------------+------------------+\n",
      "|         ITALY|                          22|                 1|\n",
      "|CAYMAN ISLANDS|                          21|                 1|\n",
      "|   BAHAMAS THE|                          16|                 1|\n",
      "| UNITED STATES|                          16|                 1|\n",
      "|         ITALY|                          14|                 1|\n",
      "|         JAPAN|                           9|                 1|\n",
      "|         INDIA|                           6|                 1|\n",
      "|         JAPAN|                           5|                 1|\n",
      "|         GHANA|                           4|                 1|\n",
      "|        CANADA|                           4|                 1|\n",
      "+--------------+----------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grpdf6=grpdf6.filter((grpdf6.count_cons_tornado ==1) ).sort(grpdf6.sum_number_of_tornado_on_day.desc())\n",
    "grpdf6.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: Italy had max 22 days of tornado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
